{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Centric AI Modeling using Snowflake and Amazon SageMaker\n",
    "\n",
    "This notebook guides you through a Data Centric machine learning (ML) development process using Snowflake and Amazon SageMaker. We demonstrate the use case through a credit-risk analysis use case.\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "* How to use the Snowflake connector for Amazon SageMaker DataWrangler.\n",
    "* How to train a model using a AWS Marketplace algorithm, Autogluon.\n",
    "* How to enrich your ML dataset with data from the Snowflake Data Marketplace.\n",
    "* How to iterate on your model design and data prep flows with the provided tools.\n",
    "* How to deploy a production scoring pipeline.\n",
    "\n",
    "**How to run it:**\n",
    "\n",
    "This notebook was designed for Amazon SageMaker Studio, and to run on the Snowflake kernel that has been customized for this workshop. CloudFormation [templates](https://github.com/dylan-tong-aws/snowflake-sagemaker-workshops) have been provided for you to setup the workshop environment including the creation of the Snowflake SageMaker kernel.\n",
    "\n",
    "Refer to this [DockerFile](https://github.com/aws-samples/amazon-sagemaker-kernel-builder/blob/main/kernels/snowflake/Dockerfile) if you like to know what dependencies are baked into the kernel. The kernel is created and integrated using the [Amazon SageMaker Studio Kernel Builder solution](https://github.com/aws-samples/amazon-sagemaker-kernel-builder)\n",
    "\n",
    "**Have feedback?** <br>\n",
    "Contact: [Dylan Tong](mailto:dylatong@amazon.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pre-requisites\n",
    "\n",
    "1. **Setup Environment** using the provided CloudFormation [templates](https://github.com/dylan-tong-aws/snowflake-sagemaker-workshops). You can use the following launch button if you haven't completed this step yet.\n",
    "\n",
    "    <a href=\"https://console.aws.amazon.com/cloudformation/home?region=region#/stacks/new?stackName=snowflake-sagemaker-credit-risk-workshop&templateURL=https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/VHOL_Snowflake_Data_Wrangler/V2/cft/workshop-setup-no-studio.yml\"/> ![Existing SageMaker Studio Environment](./images/deploy-to-aws.png) \n",
    "    \n",
    "    The button above will launch a CloudFormation template that creates IAM permissions, a Snowflake [Storage Integration](https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html), and a custom Amazon SageMaker Studio Kernel that pre-installs libraries like the Snowflake Python Connector. The template automates a lot of functionality to simplify this workshop. You need to relog into Amazon SageMaker Studio to see the \"snowflake-workshop\" kernel.\n",
    "    \n",
    "    If you are running this lab with an existing Amazon SageMaker Studio environment, you should ensure that you're running the **latest versions of Amazon SageMaker Studio and DataWrangler**. This involves restarting the applications to force an upgrade. Follow the instructions provided in the **[documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.html)**.\n",
    "\n",
    "2. Run the following cells to import the required libraries and set the global variables. Make sure that your notebook is running in the \"snowflake-workshop\" kernel environment. If you made changes to the CFT also copy the name of the S3 bucket that will be used by SageMaker.\n",
    "\n",
    "   <img src=\"./images/snowflake-kernel.png\" align=\"left\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import AlgorithmEstimator, get_execution_role\n",
    "\n",
    "import utils.algo\n",
    "import utils.dw\n",
    "from utils.ag_model import AutoGluonTraining, AutoGluonInferenceModel\n",
    "from workflow.pipeline import BlueprintFactory\n",
    "from utils.trust import ModelInspector\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region     = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket     = f\"snowflake-sagemaker-{region}-{account_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Configure Permissions\n",
    "---\n",
    "\n",
    "#### 1.1 Provide Access to [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/)\n",
    "\n",
    "Later in this lab, you will need to provide database credentials. We need to provide your Amazon SageMaker Studio environment permission to access AWS Secrets Manager so that the credentials are stored securely. \n",
    "\n",
    "We do this by **attaching the [SecretsManagerReadWrite](https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/SecretsManagerReadWrite$jsonEditor) managed policy** to your Amazon SageMaker Studio's execution role. The authentication method used in this lab requires your Amazon SageMaker environment to have access to [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/). If you provisioned this Amazon SageMaker Studio environment using the provided CloudFormation templates, this step has already been done for you.\n",
    "\n",
    "If not, run the following cell to generate a direct link to the IAM role and attach the managed policy. Your execution role should look like the following:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/secret-manager-policy.png\" align=\"left\" width=\"65%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolename = get_execution_role().split(\"/\")[-1]\n",
    "exec_role_url = f\"https://console.aws.amazon.com/iam/home?#/roles/{rolename}\"\n",
    "md(f\"IAM console redirect: {exec_role_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build your Data Prep Flow\n",
    "---\n",
    "\n",
    "Next, we'll use the Snowflake connector for Data Wrangler to access our data and describe a pipeline that prepares our data for machine learning (ML) training. \n",
    "\n",
    "#### 2.1 Create a new Data Wrangler Flow\n",
    "\n",
    "<img src=\"./images/create-data-flow.png\" width=30% align=\"left\" img/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.2. Create a Snowflake Connection\n",
    "\n",
    "Select Snowflake from the data source dropdown. </br>\n",
    "\n",
    "<img src=\"./images/create-snowflake-connection.png\" width=75% align=\"left\" img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Configure the connection with information about your account and storage integration. </br>\n",
    "\n",
    "Use the Storage Integration name identified in the Snowflake worksheet. \n",
    "You can use the Snowflake username and password, or you can use the AWS Secret that was created by the Snowflake Storage Integration CloudFormation Template. Copy the Secret ARN to paste in from the [Secrets Manager Console](https://console.aws.amazon.com/secretsmanager/home). </br>\n",
    "\n",
    "\n",
    "<img src=\"./images/configure-snowflake-connection.png\" width=45% align=\"left\" img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.3 Explore your Snowflake Data\n",
    "\n",
    "1. Select your data warehouse, database and schema. Warehouse - ML_WH; Database - ML_LENDER_DATA, Schema - ML_DATA\n",
    "2. Run SELECT * FROM ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML. The data set will be sampled by default.\n",
    "\n",
    "This data set consists of a year of loan data derived from [LendingClub](https://www.lendingclub.com/) data. It has been augmented with unemployment rate data provided by [Knoema](https://knoema.com/) from the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/). In this lab, we will demonstrate the data centric approach to improving machine learning model by showing how data from the Snowflake Marketplace can improve prediction performance. \n",
    "\n",
    "<img src=\"./images/query-and-explore-snowflake.png\" width=80% align=\"left\" img/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.4 Refine your Features\n",
    "\n",
    "With a bit of intuition and experience, you should be able to quickly spot some data columns that are unlikely to be good features. For instance, **LOAN_ID** is the unique identifier, and intuitively, we know that it has no meaningful correlation with loan defaults. On the other hand, **LOAN_AMNT** (*loan amount*) and **GRADE**, has potential. Large loans might bear greater risk. Similarly, we expect Grade F loans be riskier than Grade A ones. Thus, the former would help predict defaults. The machine learning algorithms can learn from these patterns and build a model that can predict the risk of defaults.\n",
    "\n",
    "**Run** the following query to acquire a filtered list of potential features. Next, click the **Import** button and name your training dataset. \n",
    "\n",
    "Note: \n",
    "- You also have the alternative option to drop columns as part of your DataWrangler data prep flow.\n",
    "- We're using Snowflake's sampling functionality to create a train/test set split. This query generates a repeatable 80% sampling of our data.\n",
    "\n",
    "**SELECT** </br>\n",
    "   >LOAN_ID, </br>\n",
    "    LOAN_AMNT, </br> \n",
    "    FUNDED_AMNT, </br>\n",
    "    TERM, </br>\n",
    "    INT_RATE, </br>\n",
    "    INSTALLMENT, </br>\n",
    "    GRADE, </br>\n",
    "    SUB_GRADE, </br>\n",
    "    EMP_LENGTH, </br>\n",
    "    HOME_OWNERSHIP, </br>\n",
    "    ANNUAL_INC, </br>\n",
    "    VERIFICATION_STATUS, </br>\n",
    "    PYMNT_PLAN, </br>\n",
    "    PURPOSE, </br>\n",
    "    ZIP_SCODE, </br>\n",
    "    DTI, </br>\n",
    "    DELINQ_2YRS, </br>\n",
    "    EARLIEST_CR_LINE, </br>\n",
    "    INQ_LAST_6MON, </br>\n",
    "    MNTHS_SINCE_LAST_DELINQ, </br>\n",
    "    MNTHS_SINCE_LAST_RECORD, </br>\n",
    "    OPEN_ACC, </br>\n",
    "    PUB_REC, </br>\n",
    "    REVOL_BAL, </br>\n",
    "    REVOL_UTIL, </br>\n",
    "    TOTAL_ACC, </br>\n",
    "    LOAN_DEFAULT, </br>\n",
    "    ISSUE_MONTH </br>\n",
    "    \n",
    "**FROM** ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML </br>\n",
    "sample block (80) REPEATABLE(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.5 Profile your Data\n",
    "\n",
    "Data profiling and analysis is often a good place to start before you begin building your data preparation flow. Follow the steps illustrated by the video to create a histogram to analyze the distribution of loan defaults. **LOAN_DEFAULT** is the feature to plot.\n",
    "\n",
    "![Loan Default Distribution](./images/create-histogram.gif)\n",
    "\n",
    "\n",
    "Typical of loan default data, the dataset is skew. There are less default cases than successful ones. Our analysis helps us confirm that the skew is manageable. An AutoML algorithm will apply the appropriate mitigation techniques for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.6 Apply Feature Transforms\n",
    "\n",
    "There are features that require transformations before the data can be trained. Later on, you will use an AutoML algorithm to train a model. The algorithm automates a great deal of feature engineering. Nonetheless, there is some data preparation that cannot be automated. We will go through the exercise of using SageMaker DataWrangler to transform the **INT_RATE** column. \n",
    "\n",
    "First, select the tail of the flow and select **Add transform**. </br>\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=./images/create-transform.png width=\"60%\" align=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INT_RATE** is an example of column that requires human input to properly process. This column is stored as a string type. ML algorithms only on numerical data. AutoML algorithms typically provide automation. They will detect string type columns and convert them accordingly. However, generally, they will assume that this column is categorical and apply [one-hot encoding](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-cat-encode), which isn't an effective transformaton for this feature. Instead, this data attribute should be treated as a continuous numerical feature. \n",
    "\n",
    "**TERM**, on the other hand, is seemingly similar but it could be left as a string. The dataset consists of 36 and 60 month terms. If you leave it as is, an AutoML algorithm will automically one-hot encode this column.\n",
    "\n",
    "We can transform INT_RATE into a contnuous numerical value by removing the % sign and casting the string into a float type.\n",
    "\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Search and Edit\"** category.\n",
    "* Under **Input Columns** select **INT_RATE**\n",
    "* Enter the \"%\" sign in the **Pattern** text box.\n",
    "* Leave the text box for **Replacement string** empty. Note that will need to add a character and delete it in order to trigger the **Preview** button to light up.\n",
    "* Click on the **Preview** button to preview the results and finalize the transformation by clicking on the **Add** button.\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=./images/ft-search-replace.png width=\"70%\" align=\"left\"/>\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we cast the column's data type from String to Float:\n",
    "\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Parse column as type\"** category.\n",
    "* Under **Column** select **INT_RATE**.\n",
    "* Select the **Float** option on the **\"To:\"** column.\n",
    "* Click on the **Preview** button to preview the changes and finalize the transformation by clicking on the **Add** button.\n",
    "\n",
    "\n",
    "<img src=./images/ft-parse-type.png width=\"70%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The **VERIFICATION_STATUS** column has a minor data quality issue. This feature is effectively a boolean, but the verified status is represented as two values. This transformation requires custom logic. In such cases, we can run a custom script using a **Custom Transform**. \n",
    "\n",
    "Copy the following PySpark script:\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import LongType\n",
    "\n",
    "    def categories(status) : \n",
    "      if not status :\n",
    "        return None\n",
    "      elif status == \"not verified\" :    \n",
    "        return 0\n",
    "      elif status == \"VERIFIED - income\":\n",
    "        return 1\n",
    "      elif status == \"VERIFIED - income source\":\n",
    "        return 1\n",
    "      else :\n",
    "        return None\n",
    "\n",
    "    bucket_udf = udf(categories, LongType()) \n",
    "    df = df.withColumn(\"VERIFIED\", bucket_udf(\"VERIFICATION_STATUS\")).drop(\"VERIFICATION_STATUS\")\n",
    "\n",
    "Apply the script by creating a **Custom Transform**:\n",
    "\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Custom transform\"** category.\n",
    "* Under **Column** select **INT_RATE**.\n",
    "* Select the **Float** option on the **\"To:\"** column.\n",
    "* Click on the **Preview** button to preview the changes and finalize the transformation by clicking on the **Add** button.\n",
    "\n",
    "\n",
    "\n",
    "<img src=./images/ft-custom-transform.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Finally, drop the **LOAN_ID** column. This is a unique identifier for each loan. It will only add noise to the training data. \n",
    "\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Manage columns\"** category.\n",
    "* The **Transform** dialog should have **\"Drop column\"** selected.\n",
    "* Under **\"Columns to drop\"**, select **LOAN_ID**.\n",
    "* Click on the **Preview** button to preview the changes and finalize the transformation by clicking on the **Add** button.\n",
    "\n",
    "\n",
    "<img src=\"images/ft-drop-loanid.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Click on **\"back to data flow\"**. You should see the four transforms steps at the tail of your data prep flow. \n",
    "\n",
    "<img src=./images/flow-w-transforms.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.7 Data Validation\n",
    "\n",
    "It is best practice to perform data validation before model training. DataWrangler provides useful reports to faciliate data bias and target leakage analysis. Our use case, loan default prediction, has legal and ethical risk considerations. For instance, [data bias](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-bias-report) can result in models that could put certain demographics at a disadvantage. For instance, a loan default model could unfairly reject a disproportionate number of minority group loan applications without merit as a consequence of training on flawed data.\n",
    "\n",
    "You also want to avoid [target leakage](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-analysis-target-leakage). Target leakage occurs when you accidently train a model with features that are not available in production. As a consequence, you end up with a deceptively effective model in development that causes problems in production. You can mitigate production issues by performing target leakage analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a Target Leakage report as demonstrated by the video below. Use the following settings:\n",
    "\n",
    "- **Max features:** 30\n",
    "- **Problem Type:** Classification\n",
    "- **Target:** LOAN_DEFAULT\n",
    "\n",
    "\n",
    "![Target Leakage Report](./images/target-leakage-report.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The report indicates that there is no target leakage risk. It does detect some potentially redundant features. The AutoML algorithm that you will use will mitigate redundant features. As an optional exercise, you can run experiments and determine whether these potentially redundant features effect model performance.\n",
    "\n",
    "<img src=./images/target-leakage-results.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Create a Bias Report as demonstrated by the following video. Use the following settings:\n",
    "\n",
    "- **Select the column your model predicts (target):** LOAN_DEFAULT\n",
    "- **Is your predicted column a value or threshold?:** Value\n",
    "- **Predicted value(s):** 1\n",
    "- **Select the column to analyze for bias:** ZIPS_CODE\n",
    "- **Is your column a value or threshold?:** Value\n",
    "- **Column value(s) to analyze for bias:** 200xx;207xx;206xx;900xx;100xx;941xx\n",
    "\n",
    "![Bias Report](./images/create-bias-report.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data does not have any obvious sensitive attributes like gender and race. However, it does contain zip codes. It's possible that we have a flawed dataset with an abnormal number of loan defaults in minority communities. This might not represent the actual distribution. Regardless, this situation could create a model that is biased against minorities resulting in legal risk.\n",
    "\n",
    "The report does not reveal any salient data bias issues.\n",
    "\n",
    "<img src=./images/bias-report-results.png width=\"65%\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Prototype your Model\n",
    "\n",
    "Amazon SageMaker provides a broad range of remote training services that can help you scale your ML experimentation, training and tuning process. But before you commit to a long running process, it maybe desireable to explore different combinations of candidate features and be able to rapidly iterate on a few prototypes.  \n",
    "\n",
    "#### 3.1 Create a Quick Model Report\n",
    "\n",
    "Amazon Data Wrangler provides a **[Quick Model](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-quick-model)** report which can serve as a prototyping mechanism. The report will sample your dataset, process your flow and generates a Random Forest Model. The report provides model and feature importance scores to help you assess:\n",
    "\n",
    "* What features are most impactful?\n",
    "* Does your data have enough predictive signals to produce a practical model?\n",
    "* Are your changes to your dataset leading to improvements?\n",
    "\n",
    "Navigate to the Analysis panal from the tail end of your flow—as you did in the previous section. \n",
    "\n",
    "Configure your report:\n",
    "* **Analysis type:** Quick Model\n",
    "* **Analysis name:** Quick Test\n",
    "* **Label:** LOAN_DEFAULT\n",
    "\n",
    "It will take about 5 minutes to generate a report like the following:\n",
    "\n",
    "<img src=\"./images/quick-model-iter1.png\" width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of the feature importance ranking in the bar chart. This gives you an *approximation* of which features have strong predictive signals. The F1 score of <span style=\"color:yellow\">**0.6454**</span> is not great. However, you can expect better results with a complete training and tuning process. The score tells you that your dataset has potential to produce a practical model.\n",
    "\n",
    "Note that your model's performance might differ slightly due to the random sampling that is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Iterate, Experiment and Improve\n",
    "\n",
    "You can improve your model's performance through further feature engineering and improvements to your dataset. Next, you will do just that by enriching your dataset with data obtained from the Snowflake's Data Marketplace.\n",
    "\n",
    "In the following sections, we'll be modifying our existing flow. In practice, you should version control your .flow files first through the [Git integration](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-git-repo.html)\n",
    "\n",
    "#### Step 4.1 Explore and Extract Candidate Features from the Data Marketplace\n",
    "\n",
    "Add a new data source to your existing flow. Select the **Import** sub tab and click on the Snowflake icon. Run the following query to extract the unemployment rate data that you obtained from the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/).\n",
    "\n",
    "**SELECT**\n",
    ">UNEMPLOYMENT_RATE, </br>\n",
    "LOAN_ID </br>\n",
    "\n",
    "**FROM** ML_LENDER_DATA.ML_DATA.UNEMPLOYMENT_DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 4.2 Augment your Dataset\n",
    "\n",
    "Next, you're going to merge the two datasets. There are many ways to do this. You could have perform this entirely using Snowflake. In this lab, you'll learn how to perform this merge through DataWrangler. This method provides you with a visualization of the modified flow and the change can be version control within your Git repository.\n",
    "\n",
    "\n",
    "First, **Delete** the last transformation from the original flow, so that we have **LOAN_ID** available in the original dataset.\n",
    "\n",
    "![Delete Step](./images/delete-step.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, replicate the steps in the following video to merge the unemployment rate feature into your dataset.\n",
    "\n",
    "1. Click on the end of the original flow and select the **Join** operator.\n",
    "2. Select the end of the other flow.\n",
    "3. Click on the button labeled **Configure**. \n",
    "3. Select **Left Outer** as the **Join Type**.\n",
    "4. Select **LOAN_ID** for both the **Left** and **Right** join keys.\n",
    "\n",
    "![Join Datasets](./images/join-and-enrich-flow.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, discard the join keys. Same as before, use the **\"Manage columns\"** transform to drop columns. \n",
    "\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Manage columns\"** category.\n",
    "* The **Transform** dialog should have **\"Drop column\"** selected.\n",
    "* Under **\"Columns to drop\"**, select **LOAN_ID_0**.\n",
    "* Click on the **Preview** button to preview the changes and finalize the transformation by clicking on the **Add** button.\n",
    "* Repeat the above steps for the column **LOAN_ID_1**.\n",
    "\n",
    "\n",
    "<img src=\"./images/ft-drop-loanid0.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ft-drop-loanid1.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **[OPTIONAL]** Step 4.3 Re-validate your Dataset\n",
    "\n",
    "In practice, you should re-validate your dataset since it has been modified. The Target Leakage report calculates the correlation between your features and the target variable. In effect, it provides you with an idea of how likely your new feature will improve your model. The report should present the new feature, **UNEMPLOYMENT_RATE**, as the feature with the highest predictive potential.\n",
    "\n",
    "<img src=\"./images/target-leakage-report-w-unemployment.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4.4 Evaluate your Dataset Modifications\n",
    "\n",
    "Next, we're going to evaluate whether our new feature is beneficial. We will use the Quick Model report again to get a quick assessment. Note that in practice, you might want to be more thorough and fully train and tune a model on some your dataset iterations so that you have a reliable baseline. For the sake of demonstration, we use the Quick Model report exclusively.\n",
    "\n",
    "Create a new **Quick Model** report to assess the impact of your modifications. The results should be similiar to the following:\n",
    "\n",
    "<img src=\"./images/quick-model-iter2.png\" width=\"65%\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of key takeaways:\n",
    "* UNEMPLOYMENT_RATE is clearly ranked as the most important feature. \n",
    "* The F1 score increased to <span style=\"color:lightgreen\">**0.78**</span> from 0.645.\n",
    "\n",
    "This tells us that we are likely heading in the right direction. We added a feature that generated noteable improvements to the \"quick model\" and the new feature had the greatest impact.\n",
    "\n",
    "Note that due to stochastic processes, your results might be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Generate your Dataset\n",
    "\n",
    "We are now ready to fully train and tune a model. First, we need to generate our datasets by executing the data flow that we've created.\n",
    "\n",
    "#### 5.1 Export Your Data Flow\n",
    "\n",
    "DataWrangler supports multiple ways to [export](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-export.html) the flow for execution. In this lab, you will select the option that generates a notebook that can be run to execute the flow as a [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) job. This is the simplest option. The other options offer capabilities that you might value in production deployments.\n",
    "\n",
    "* Click on the **\"+\"** symbol at the end of your data flow and mouse over the **\"Export to\"** option.\n",
    "* Click on the **\"Amazon S3 (via Jupyter Notebook)\"** in the sub dialog window.\n",
    "\n",
    "\n",
    "![Export Script](./images/data-flow-export.png)\n",
    "\n",
    "\n",
    "---\n",
    "#### 5.2 Execute the Data Flow\n",
    "\n",
    "* Follow the steps outlined in the **generated notebook**. \n",
    "* **Run the cells and wait for the processing job to complete**. \n",
    "* Copy the output S3 URI of the processed dataset. \n",
    "\n",
    "\n",
    "The S3 URI will look similar to: *s3://(YOUR BUCKET)/export-flow-23-23-17-34-6a8a80ec/output/data-wrangler-flow-processing-23-23-17-34-6a8a80ec*. It will appear in the following cell of the generated notebook:\n",
    "\n",
    "![Export Script](./images/prep-data-location.png)\n",
    "\n",
    "\n",
    "Set the variable **PREP_DATA_S3** in the following cell to that S3 URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREP_DATA_S3 = \"REPLACE ME!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Train Your Model\n",
    "\n",
    "Your data is now ready for training a model. We'll be using an AutoML tooklikt, [Autogluon](https://auto.gluon.ai/stable/tutorials/tabular_prediction/index.html), to automate the model centric optimizations like algorithm selection as well as advanced techniques like ensembling. \n",
    "\n",
    "The default settings for Autogluon will suffice for generating a high-performing model. However, there are configuratons that you can experiment with to fine tune your model. You can learn more from the advanced [tutorial](https://auto.gluon.ai/dev/tutorials/tabular_prediction/tabular-indepth.html) provided by the Autogluon development team.\n",
    "\n",
    "----\n",
    "\n",
    "Run the following cell below to configure an AutoGluon model generation job to run as a remote process that is managed by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri = utils.dw.get_data_uri(PREP_DATA_S3)\n",
    "s3_prefix = f\"autogluon_sm/{sagemaker.utils.sagemaker_timestamp()}\"\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=data_uri, content_type='csv')\n",
    "\n",
    "autogluon_model = AutoGluonTraining(\n",
    "    role=role,\n",
    "    entry_point=\"scripts/train.py\",\n",
    "    region=region,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    framework_version=\"0.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    base_job_name=\"autogluon-tabular-train\",\n",
    ")\n",
    "\n",
    "ag_config = autogluon_model.sagemaker_session.upload_data(\n",
    "                path=os.path.join(\"scripts\", \"train-config.yaml\"), \n",
    "                key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell will launch the remote training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogluon_model.fit({\"config\": ag_config,'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Review the output generated by the training job. The top performing model generated by Autogluon should, again, be the WeightedEnsemble. Your model's AUC score will be in the vicinity of <span style=\"color:lightgreen\">**0.88**</span>. \n",
    "\n",
    "If you had created a baseline model with the previous dataset version, you would have obtained an AUC score around 0.84. Thus, the data enrichment yielded an uplift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **[OPTIONAL]** Step 7: Deploy\n",
    "\n",
    "You can serve your predictions in a couple of ways. You could deploy the model as a [real-time hosted endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html) on SageMaker and integrate it with Snowflake as an [External Function](https://docs.snowflake.com/en/sql-reference/external-functions-creating-aws.html). This will enable you to query your predictions in real-time and minimize data staleness.\n",
    "\n",
    "Alternatively, you can pre-calculate your predictions as a transient batch process. In the following section, you will use [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) to do just that. When your use case allows you to pre-calculate predictions, Batch Transform is a good option. Batch Transform design to scale-out and is optimized for throughput while the real-time endpoints are designed for low latency. Generally, Batch Transform is the cost efficient option as you are only charged for the resources used by the transient batch job. You should run your Batch Transform job as part of an automated workflow in production.\n",
    "\n",
    "In the following sections we are going to deploy our model as a batch inference pipeline. The pipeline is designed to consume data from Snowflake, process it using our DataWrangler flow and then pre-calculate predictions using our trained model and Batch Transform. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7.1 Modify your Data Prepartion flow for Inference\n",
    "\n",
    "You are going to use your model to generate predictions and a credit risk score on unseen data. You can re-use your data preparation flow, but you will need to update your data source.\n",
    "\n",
    "* Make a copy of your flow file. In practice, you should also commit this to version control. \n",
    "* Assign **INFERENCE_FLOW_NAME** with the name of your *.flow* file and run the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_FLOW_NAME = \"inference_flow_loan.flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next edit the query for your loan origination data source. We will use the 20% data sample that we held out for the purpose of demonstration. The query is as follows.\n",
    "\n",
    "   **SELECT** </br>\n",
    "     &nbsp; L1.LOAN_ID, </br>\n",
    "     &nbsp; L1.LOAN_AMNT, </br>\n",
    "     &nbsp; L1.FUNDED_AMNT, </br>\n",
    "     &nbsp; L1.TERM, </br>\n",
    "     &nbsp; L1.INT_RATE, </br>\n",
    "     &nbsp; L1.INSTALLMENT, </br>\n",
    "     &nbsp; L1.GRADE, </br>\n",
    "     &nbsp; L1.SUB_GRADE, </br>\n",
    "     &nbsp; L1.EMP_LENGTH, </br>\n",
    "     &nbsp; L1.HOME_OWNERSHIP, </br>\n",
    "     &nbsp; L1.ANNUAL_INC, </br>\n",
    "     &nbsp; L1.VERIFICATION_STATUS, </br>\n",
    "     &nbsp; L1.PYMNT_PLAN, </br>\n",
    "     &nbsp; L1.PURPOSE, </br>\n",
    "     &nbsp; L1.ZIP_SCODE, </br>\n",
    "     &nbsp; L1.DTI, </br>\n",
    "     &nbsp; L1.DELINQ_2YRS, </br>\n",
    "     &nbsp; L1.EARLIEST_CR_LINE, </br>\n",
    "     &nbsp; L1.INQ_LAST_6MON, </br>\n",
    "     &nbsp; L1.MNTHS_SINCE_LAST_DELINQ, </br>\n",
    "     &nbsp; L1.MNTHS_SINCE_LAST_RECORD, </br>\n",
    "     &nbsp; L1.OPEN_ACC, </br>\n",
    "     &nbsp; L1.PUB_REC, </br>\n",
    "     &nbsp; L1.REVOL_BAL, </br>\n",
    "     &nbsp; L1.REVOL_UTIL, </br>\n",
    "     &nbsp; L1.TOTAL_ACC, </br>\n",
    "     &nbsp; L1.LOAN_DEFAULT, </br>\n",
    "     &nbsp; L1.ISSUE_MONTH </br>\n",
    "    **FROM** ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML **AS** L1 </br>\n",
    "     &nbsp;**LEFT OUTER JOIN**  </br>\n",
    "     &nbsp;(**SELECT** * FROM ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML **sample block (80) REPEATABLE(100)**) **AS** L2 </br>\n",
    "     &nbsp;**ON** L1.LOAN_ID = L2.LOAN_ID </br>\n",
    "    **WHERE** L2.LOAN_ID **IS NULL** </br>\n",
    "\n",
    "The following video demonstrates how to modify your query.\n",
    "\n",
    "![Edit Query](./images/flow-edit-query.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this flow is for use in production to prep data from new loan applications. This data is then passed through your model to make predications about the default risks of these new applicants. \n",
    "\n",
    "For testing purposes, we're using the 20% of the data set we didn't use for training and we're including the LOAN_DEFAULT attribute so that we can evaluate our models and perform error analysis. We're going to move the the LOAN_DEFAULT column to the first position in our dataset, so that it's easier for us to filter this data out of the model input and merge it with the model's predictions.\n",
    "\n",
    "* Select **Add transform** at the end of your data flow.\n",
    "* Click on the orange button labeled **Add step**\n",
    "* Click on the **\"Manage columns\"** category.\n",
    "* Select **Move column** under the **Transform** dialog.\n",
    "* Select **Move to start** under the **Move type** dialog.\n",
    "* Select **LOAN_DEFAULT** under the **Column to move** dialog. \n",
    "* Click on the **Preview** button to preview the changes and finalize the transformation by clicking on the **Add** button.\n",
    "\n",
    "\n",
    "<img src=\"images/move-column.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2 Re-export and Re-factor your Flow as a Pipeline\n",
    "\n",
    "Your goal is to deploy a credit-risk scoring pipeline into production. DataWrangler provides the option to deploy your flow as an Amazon SageMaker Pipeline to facilitate this:\n",
    "\n",
    "<img src=\"./images/export-flow-as-pipeline.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you will need to refactor the exported script. This has been done for you, so all you need to do is find locate the export node-id. Each step in your data flow is a unique node and the export script is dependent on the node that you select for export. The node id should look like the image below.\n",
    "\n",
    "Copy your node id, assign **FLOW_NODE_ID** to this value and run the following cell.\n",
    "\n",
    "<img src=\"./images/export-node-id.png\" width=\"60%\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_NODE_ID = \"REPLACE ME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you like to view the refactored script. The exported pipeline script has been refactored such that it runs a Batch Transform job after the data preparation processing job to generate the credit-risk scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"./workflow/pipeline.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to execute your batch scoring pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_TYPE = \"ml.c5.2xlarge\"\n",
    "batch_output_prefix = \"batch/out\"\n",
    "batch_s3_output_uri = f\"s3://{bucket}/{batch_output_prefix}\"\n",
    "\n",
    "predictor = AutoGluonInferenceModel(\n",
    "    model_data= autogluon_model.model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=INFERENCE_SERVER_TYPE,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"tabular_serve.py\",\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"dw_output_name\"              : FLOW_NODE_ID,\n",
    "    \"dw_instance_count\"           : 1,\n",
    "    \"dw_instance_type\"            : \"ml.m5.4xlarge\",\n",
    "    \"dw_flow_filepath\"            : \"\",\n",
    "    \"dw_flow_filename\"            : INFERENCE_FLOW_NAME,\n",
    "    \"dw_volume_size_in_gb\"        : 30,\n",
    "    \"dw_output_content_type\"      : \"CSV\",\n",
    "    \"dw_enable_network_isolation\" : False,\n",
    "    \"dw_source_bucket\"            : bucket,\n",
    "    \"batch_instance_type\"         : INFERENCE_SERVER_TYPE,\n",
    "    \"batch_instance_count\"        : 1,\n",
    "    \"batch_s3_output_uri\"         : batch_s3_output_uri,\n",
    "    \"wf_instance_type\"            : \"ml.m5.4xlarge\",\n",
    "    \"wf_instance_count\"           : 1,\n",
    "    \"sm_estimator\"                : predictor,\n",
    "}\n",
    "\n",
    "bpf      = BlueprintFactory(config)\n",
    "pipeline = bpf.get_batch_pipeline()\n",
    "\n",
    "execution = pipeline.start()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can monitor the status of your pipeline from Amazon SageMaker Studio. The following video demonstrates how to do this.\n",
    "\n",
    "![Pipeline Status](./images/pipeline-status.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit-risk prediction data is small enough for us to load into a local pandas dataframe. Run the following cell to preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "output_uri = utils.dw.get_data_uri(batch_s3_output_uri)\n",
    "results = pd.read_json(output_uri).drop([1,2], axis=1).rename(columns={0:\"label\",3:\"p_default\"})\n",
    "results[\"predictions\"] = (results[\"p_default\"] > threshold).astype(int)\n",
    "\n",
    "cols            = [\"label\",\"predictions\",\"p_default\"] \n",
    "results_file    = \"results.csv\"\n",
    "results_prefix  = \"results\"\n",
    "\n",
    "results.to_csv(path_or_buf=results_file, columns = cols, index=False)\n",
    "S3Uploader.upload(results_file, f\"s3://{bucket}/{results_prefix}\")\n",
    "\n",
    "results[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our hold-out test set to generate the scores. Let's evaluate our model performance with the provided utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector_params = {\n",
    "    \"workspace\": bucket,\n",
    "    \"drivers\":{\n",
    "        \"db\": boto3.client(\"s3\"),\n",
    "        \"dsmlp\": boto3.client(\"sagemaker\"),\n",
    "    },\n",
    "    \"prefixes\": {\n",
    "        \"results_path\": results_prefix,\n",
    "        \"bias_path\": None,\n",
    "        \"xai_path\": None,\n",
    "    },\n",
    "    \"results-config\":{\n",
    "        \"gt_index\": 0,\n",
    "        \"pred_index\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "inspector = ModelInspector.get_inspector(inspector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "inspector.display_interactive_cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lastly, we're going to use the Snowflake Python connector to load our predictions into Snowflake to drive credit-risk analysis. This is only practical for smaller data sets. For larger data sets the Snowflake [COPY](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html) command can be used to load data directly from S3.\n",
    "\n",
    "In practice, the process of loading the predictions from S3 into Snowflake should be part of your production scoring pipeline. Snowflake provides a service called [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro.html) that autoamtically can load data from S3 to Snowflake and will automatically scale based on the data volume, with no manamgement required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already used the AWS Secrets Manager to store your Snowflake credentials. Go to the [Secrets Manager Console](https://console.aws.amazon.com/secretsmanager/home). Select the Snowflake Secret and copy the Secret Name i.e. <span style=\"color:lightgreen\">**SnowflakeSecret-P4qyGUyk67hj**</span> in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name = \"REPLACE ME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "    return json.loads(secret)       \n",
    "    # Your code goes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "# Connecting to Snowflake using the default authenticator\n",
    "\n",
    "# Get credentials from Secrets Manager \n",
    "snowcreds = get_secret()\n",
    "\n",
    "\n",
    "ctx = snowflake.connector.connect(\n",
    "  user=snowcreds[\"username\"],\n",
    "  password=snowcreds[\"password\"],\n",
    "  account=snowcreds[\"accountid\"],\n",
    "  warehouse='ML_WH',\n",
    "  database='ML_LENDER_DATA',\n",
    "  schema='ML_DATA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# Write the predictions to the table named \"ML_RESULTS\".\n",
    "success, nchunks, nrows, _ = write_pandas(ctx, results[cols], 'ML_RESULTS', quote_identifiers=False)\n",
    "\n",
    "display(nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Clean up\n",
    "\n",
    "Congratulations! You've completed the lab. You can delete the active resources created for the lab by deleting the CloudFormation templates. Also check for any S3 buckets that were created, empty them and delete the buckets.\n",
    "\n",
    "Follow the steps in Snowflake Worksheet (SQL Script) to delete all Snowflake resources that were created.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (snowflake-workshop/4)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:803235869972:image-version/snowflake-workshop/4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
